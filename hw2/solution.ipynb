{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35d2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a610c280",
   "metadata": {},
   "source": [
    "## Note: the code is fully reproducible:\n",
    "- gathering a benchmark is deterministic, no randomness involved\n",
    "- all models are inferenced with temperature=0 (greedy decoding)\n",
    "- see .venv libraries at requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc24d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca10691",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee979f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac252d8bb2f746f7920b3fd740c9be6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85f00268b7245f5b5bcf96354cd94b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from consts import LLAVA_MODEL_ID, QWEN_2_5_VL_MODEL_ID\n",
    "from utils import (\n",
    "    select_different_products_by_category, download_images,\n",
    "    predict, load_results, parse_price, compute_metrics,\n",
    "    calculate_blue_and_rouge_scores, ModelsLoader\n",
    ")\n",
    "\n",
    "# Load `llava-hf/llava-1.5-7b-hf` and `Qwen/Qwen2.5-VL-3B-Instruct` into RAM\n",
    "models_loader = ModelsLoader()\n",
    "models_loader.load_model_and_processor(LLAVA_MODEL_ID)\n",
    "models_loader.load_model_and_processor(QWEN_2_5_VL_MODEL_ID);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca2f496",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd757bd7",
   "metadata": {},
   "source": [
    "# Task 0. Load a dataset & construct a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8daf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename\n",
       "meta_Clothing_Shoes_and_Jewelry    41777\n",
       "meta_Home_and_Kitchen              17326\n",
       "meta_Electronics                    7681\n",
       "meta_Automotive                     7161\n",
       "meta_Sports_and_Outdoors            6343\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"milistu/AMAZON-Products-2023\")\n",
    "dataframe: pd.DataFrame = ds[\"train\"].to_pandas()\n",
    "dataframe.filename.value_counts().iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868897e",
   "metadata": {},
   "source": [
    "It can bee seen that the filenames (which de-facto represent the parent categories) are not balanced (30 different categories).\n",
    "\n",
    "To create a representative benchmark, we need to take into account each category.\n",
    "\n",
    "Let's take 40 samples of each category if there are at least 40, otherwise take all of them.\n",
    "\n",
    "How do we select 40 samples from one category? Random sampling can be misleading, because we can sample 40 cell phones in the category `meta_Electronics`, which won't be representative.\n",
    "\n",
    "It's better to sample different products within one filename (for example, cell phone, headphones, TV etc from the category `meta_Electronics`).\n",
    "\n",
    "To achieve this, we can utilize the column `embeddings` (embeddings generated for the product using text-embedding-3-small model.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64520907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1085, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmark = pd.concat(\n",
    "    # THIS CODE IS DETERMINISTIC !!!\n",
    "    [select_different_products_by_category(dataframe, category, 40) for category in dataframe['filename'].unique()]\n",
    ")\n",
    "benchmark.reset_index(drop=True)\n",
    "\n",
    "download_images(benchmark, \"image\", save_dir=\"data/images\")\n",
    "\n",
    "benchmark.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a671ecb",
   "metadata": {},
   "source": [
    "# Benchmark size: 1085"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a40bb51",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5a9f1",
   "metadata": {},
   "source": [
    "# Task 1. Regression task: predict product price\n",
    "\n",
    "## Metrics to use: MAPE (good) + MAE and MSE (not good)\n",
    "### Why MAPE?\n",
    "Product prices can vary a lot (for example, $10, $100, $1000, etc.). Because of this, metrics like MAE and MSE are not suitable, though can be measured\n",
    "\n",
    "MAPE is the best choice because it shows how close the model's prediction is to the actual price, and it is not affected by the magnitude of the price.\n",
    "\n",
    "\n",
    "### Input: title, store, description, rating, main_category, details and image\n",
    "### Output: price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd384be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:Loaded 1085 cached results for llava-1.5-7b-hf\n",
      "INFO:utils:Predicting price for 0 samples\n",
      "INFO:utils:Saved 1085 predictions for llava-1.5-7b-hf to data/results/llava-1.5-7b-hf_price_predictions.json\n",
      "INFO:utils:Loaded 1085 cached results for Qwen2.5-VL-3B-Instruct\n",
      "INFO:utils:Predicting price for 0 samples\n",
      "INFO:utils:Saved 1085 predictions for Qwen2.5-VL-3B-Instruct to data/results/Qwen2.5-VL-3B-Instruct_price_predictions.json\n"
     ]
    }
   ],
   "source": [
    "for model_id in [LLAVA_MODEL_ID, QWEN_2_5_VL_MODEL_ID]:\n",
    "\n",
    "    model_id_name = model_id.split(\"/\")[-1]\n",
    "    results = predict(\n",
    "        models_loader=models_loader,\n",
    "        model_id=model_id,\n",
    "        benchmark=benchmark,\n",
    "        inference_batch_size=1,\n",
    "        task=\"price\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e192e68",
   "metadata": {},
   "source": [
    "### Parse results\n",
    "\n",
    "Some predictions may not be parsed correctly. Since there are few such examples, let's drop them from the benchmark for fair comparison of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c913f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL BENCHMARK SIZE FOR THE PRICE PREDICTION TASK (some samples are dropped due to invalid predictions or zero price): 1043\n"
     ]
    }
   ],
   "source": [
    "llava_results = load_results(f\"data/results/{LLAVA_MODEL_ID.split(\"/\")[-1]}_price_predictions.json\")\n",
    "qwen_results = load_results(f\"data/results/{QWEN_2_5_VL_MODEL_ID.split(\"/\")[-1]}_price_predictions.json\")\n",
    "\n",
    "llava_predictions = []\n",
    "qwen_predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "llava_predictions_per_category = defaultdict(list)\n",
    "qwen_predictions_per_category = defaultdict(list)\n",
    "ground_truth_per_category = defaultdict(list)\n",
    "\n",
    "for parent_asin in benchmark[\"parent_asin\"]:\n",
    "\n",
    "    ground_truth_price: float = benchmark.loc[benchmark[\"parent_asin\"] == parent_asin, \"price\"].values[0].item()\n",
    "    if ground_truth_price <= 0:  # this should no happen, but this happens and result in inf MAPE\n",
    "        continue\n",
    "\n",
    "    category: str = benchmark.loc[benchmark[\"parent_asin\"] == parent_asin, \"filename\"].values[0]\n",
    "\n",
    "    llava_pred: str = llava_results[parent_asin]\n",
    "    qwen_pred: str = qwen_results[parent_asin]\n",
    "\n",
    "    llava_pred_parsed: float | None = parse_price(llava_pred)\n",
    "    qwen_pred_parsed: float | None = parse_price(qwen_pred)\n",
    "\n",
    "    if not llava_pred_parsed or not qwen_pred_parsed:\n",
    "        continue # skip some (invalid) samples from the benchmark\n",
    "\n",
    "    llava_predictions.append(llava_pred_parsed)\n",
    "    qwen_predictions.append(qwen_pred_parsed)\n",
    "    ground_truth.append(ground_truth_price)\n",
    "\n",
    "    llava_predictions_per_category[category].append(llava_pred_parsed)\n",
    "    qwen_predictions_per_category[category].append(qwen_pred_parsed)\n",
    "    ground_truth_per_category[category].append(ground_truth_price)\n",
    "\n",
    "print(\"FINAL BENCHMARK SIZE FOR THE PRICE PREDICTION TASK (some samples are dropped due to invalid predictions or zero price):\", len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f789fcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                  |   MAPE (%) |   MAE |      MSE |\n",
      "|------------------------|------------|-------|----------|\n",
      "| LLaVA                  |      81.6  | 27.04 |  5669.12 |\n",
      "| Qwen                   |      59.33 | 23.6  |  5236.79 |\n",
      "| Constant (mean)        |     188.12 | 39.62 |  9254.71 |\n",
      "| Constant (median)      |      70.89 | 31.59 |  9844.34 |\n",
      "| Constant (lowest MAPE) |      56.15 | 34.08 | 10309.2  |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "llava_metrics = compute_metrics(ground_truth, llava_predictions)\n",
    "qwen_metrics = compute_metrics(ground_truth, qwen_predictions)\n",
    "\n",
    "const_predictions_mean_metrics = compute_metrics(ground_truth, [np.mean(ground_truth)] * len(ground_truth))\n",
    "const_predictions_median_metrics = compute_metrics(ground_truth, [np.median(ground_truth)] * len(ground_truth))\n",
    "\n",
    "# find the best constant prediction that minimizes MAPE\n",
    "increment: float = 0.1\n",
    "lowest_mape = 1e9\n",
    "best_c = 0\n",
    "c = 0.1\n",
    "for _ in range(10_000):\n",
    "    c += increment\n",
    "    metrics = compute_metrics(ground_truth, [c] * len(ground_truth))\n",
    "    if metrics[\"MAPE\"] < lowest_mape:\n",
    "        lowest_mape = metrics[\"MAPE\"]\n",
    "        best_c = c\n",
    "\n",
    "\n",
    "table = [\n",
    "    [\"LLaVA\"] + [llava_metrics[\"MAPE\"], llava_metrics[\"MAE\"], llava_metrics[\"MSE\"]],\n",
    "    [\"Qwen\"] + [qwen_metrics[\"MAPE\"], qwen_metrics[\"MAE\"], qwen_metrics[\"MSE\"]],\n",
    "    [\"Constant (mean)\"] + [const_predictions_mean_metrics[\"MAPE\"], const_predictions_mean_metrics[\"MAE\"], const_predictions_mean_metrics[\"MSE\"]],\n",
    "    [\"Constant (median)\"] + [const_predictions_median_metrics[\"MAPE\"], const_predictions_median_metrics[\"MAE\"], const_predictions_median_metrics[\"MSE\"]],   \n",
    "    [\"Constant (lowest MAPE)\"] + [compute_metrics(ground_truth, [best_c] * len(ground_truth))[\"MAPE\"], compute_metrics(ground_truth, [best_c] * len(ground_truth))[\"MAE\"], compute_metrics(ground_truth, [best_c] * len(ground_truth))[\"MSE\"]],   \n",
    "]\n",
    "headers = [\"Model\", \"MAPE (%)\", \"MAE\", \"MSE\"]\n",
    "print(tabulate(table, headers=headers, tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a9b97",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "- `Qwen/Qwen2.5-VL-3B-Instruct` shows better performance than `llava-hf/llava-1.5-7b-hf` on all metrics\n",
    "- 59.33% MAPE is still extremely high and further fine-tuning is necessary\n",
    "- vLLMs show better performance in comparison with the constant classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad5eb4",
   "metadata": {},
   "source": [
    "## Errors analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b453f3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Category                        |   LLaVA MAPE (%) |   LLaVA MAE |   LLaVA MSE |   Qwen MAPE (%) |   Qwen MAE |   Qwen MSE |   GT Mean |   GT std |\n",
      "|---------------------------------|------------------|-------------|-------------|-----------------|------------|------------|-----------|----------|\n",
      "| meta_Digital_Music              |            24.63 |        6.52 |      130.08 |           26.25 |       6.56 |     141.26 |     22.05 |    12.89 |\n",
      "| meta_Grocery_and_Gourmet_Food   |            57.09 |       11.63 |      320.56 |           49.21 |      11.06 |     318.69 |     26.97 |    42.5  |\n",
      "| meta_Video_Games                |            60.01 |       17.68 |      689.51 |           49.59 |      16.11 |     622.17 |     38.81 |    37.77 |\n",
      "| meta_Health_and_Household       |            62.29 |       17.82 |      920.28 |           43.4  |      17.44 |    1288.65 |     35.1  |    52.31 |\n",
      "| meta_Handmade_Products          |            64.24 |       27.46 |     3176.68 |           61.52 |      30.33 |    5114.79 |     47.28 |    72.35 |\n",
      "| meta_Baby_Products              |            66.67 |       15.1  |      966.16 |           50.19 |      20.56 |    3130.25 |     35.38 |    69.1  |\n",
      "| meta_Clothing_Shoes_and_Jewelry |            68.91 |       29.58 |     2453.8  |           63.75 |      32.13 |    2510.34 |     64.08 |    71.86 |\n",
      "| meta_Patio_Lawn_and_Garden      |            75.98 |       39.6  |     6320.37 |           62.15 |      36.84 |    6926.98 |     58.29 |   103.6  |\n",
      "| meta_Health_and_Personal_Care   |            77.13 |       15.52 |      661.51 |           50.51 |       9.66 |     155.32 |     21.75 |    16.42 |\n",
      "| meta_Sports_and_Outdoors        |            78.07 |       24.26 |     1455.63 |           41.53 |      14.98 |     497.12 |     43.85 |    52.86 |\n",
      "| meta_Automotive                 |            84.08 |       30.84 |     2399.1  |           75.75 |      26.49 |    1920.58 |     45.6  |    52.54 |\n",
      "| meta_Unknown                    |            91.11 |       25.46 |     2213.64 |           57.01 |      18.8  |    2758.44 |     35.34 |    47.57 |\n",
      "| meta_Amazon_Fashion             |            99.74 |       35.14 |     3340.99 |           84.3  |      29.53 |    3356.88 |     54.92 |    66.68 |\n",
      "| meta_Pet_Supplies               |           117.43 |       24.57 |     1659.54 |           60.76 |      15.06 |     446.17 |     35.35 |    35.86 |\n",
      "| meta_Toys_and_Games             |           127.01 |       22.98 |     1350.01 |           78.31 |      18.7  |     902.2  |     32.21 |    37.76 |\n",
      "\n",
      "Correlation between Qwen MAPE and GT std: 0.408\n",
      "Correlation between Qwen MAPE and GT mean: 0.538\n",
      "\n",
      "Correlation between LLaVA MAPE and GT std: 0.089\n",
      "Correlation between LLaVA MAPE and GT mean: 0.174\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for category, ground_truth in ground_truth_per_category.items():\n",
    "    if len(ground_truth) < 40:\n",
    "        continue\n",
    "    current_llava_metrics = compute_metrics(ground_truth, llava_predictions_per_category[category])\n",
    "    current_qwen_metrics = compute_metrics(ground_truth, qwen_predictions_per_category[category])\n",
    "    gt_mean = np.mean(ground_truth)\n",
    "    gt_var = np.std(ground_truth)\n",
    "\n",
    "    rows.append([\n",
    "        category,\n",
    "        current_llava_metrics[\"MAPE\"], current_llava_metrics[\"MAE\"], current_llava_metrics[\"MSE\"],\n",
    "        current_qwen_metrics[\"MAPE\"], current_qwen_metrics[\"MAE\"], current_qwen_metrics[\"MSE\"],\n",
    "        round(gt_mean, 2), round(gt_var, 2)\n",
    "    ])\n",
    "\n",
    "# Sort by Qwen LLaVa (index 1 in the row)\n",
    "rows.sort(key=lambda row: row[1])\n",
    "\n",
    "headers = [\n",
    "    \"Category\",\n",
    "    \"LLaVA MAPE (%)\", \"LLaVA MAE\", \"LLaVA MSE\",\n",
    "    \"Qwen MAPE (%)\", \"Qwen MAE\", \"Qwen MSE\",\n",
    "    \"GT Mean\", \"GT std\"\n",
    "]\n",
    "print(tabulate(rows, headers=headers, tablefmt=\"github\"))\n",
    "print()\n",
    "\n",
    "# compute correlation between MAPE and GT mean / GT std (seems logical to me)\n",
    "qwen_mape = [row[4] for row in rows]\n",
    "llava_mape = [row[1] for row in rows]\n",
    "gt_mean = [row[7] for row in rows]\n",
    "gt_std = [row[8] for row in rows]\n",
    "\n",
    "correlation_matrix_std_qwen = np.corrcoef(qwen_mape, gt_std)\n",
    "correlation_matrix_mean_qwen = np.corrcoef(qwen_mape, gt_mean)\n",
    "correlation_matrix_std_llava = np.corrcoef(llava_mape, gt_std)\n",
    "correlation_matrix_mean_llava = np.corrcoef(llava_mape, gt_mean)\n",
    "\n",
    "print(f\"Correlation between Qwen MAPE and GT std: {round(correlation_matrix_std_qwen[0][1], 3)}\")\n",
    "print(f\"Correlation between Qwen MAPE and GT mean: {round(correlation_matrix_mean_qwen[0][1], 3)}\")\n",
    "print()\n",
    "print(f\"Correlation between LLaVA MAPE and GT std: {round(correlation_matrix_std_llava[0][1], 3)}\")\n",
    "print(f\"Correlation between LLaVA MAPE and GT mean: {round(correlation_matrix_mean_llava[0][1], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69162d8",
   "metadata": {},
   "source": [
    "The table above shows that the score across categories vary significantly, which implies that some categories are harder to deal with (like `Toys and Games` or `Pet Supplies`), and some are pretty easy (`meta_Digital_Music`). Further analysis (to determine the underlying cause of it) is required.\n",
    "\n",
    "Also, if we look closer to the data itself, we can notice that many samples are absolutely bizarre: the price does not look correct at all.\n",
    "\n",
    "The noise in the data contributes to the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460e439",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c93536",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fd0a5",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2.1\n",
    "- Input: description, store, price, rating, main_category, details and image\n",
    "- Output: title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9c16d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:Loaded 1085 cached results for llava-1.5-7b-hf\n",
      "INFO:utils:Predicting title for 0 samples\n",
      "INFO:utils:Saved 1085 predictions for llava-1.5-7b-hf to data/results/llava-1.5-7b-hf_title_predictions.json\n",
      "INFO:utils:Loaded 1085 cached results for Qwen2.5-VL-3B-Instruct\n",
      "INFO:utils:Predicting title for 0 samples\n",
      "INFO:utils:Saved 1085 predictions for Qwen2.5-VL-3B-Instruct to data/results/Qwen2.5-VL-3B-Instruct_title_predictions.json\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava-hf/llava-1.5-7b-hf</th>\n",
       "      <td>0.089</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-VL-3B-Instruct</th>\n",
       "      <td>0.102</td>\n",
       "      <td>0.419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              BLEU  ROUGE-L\n",
       "Model                                      \n",
       "llava-hf/llava-1.5-7b-hf     0.089    0.385\n",
       "Qwen/Qwen2.5-VL-3B-Instruct  0.102    0.419"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_id in [LLAVA_MODEL_ID, QWEN_2_5_VL_MODEL_ID]:\n",
    "\n",
    "    model_id_name = model_id.split(\"/\")[-1]\n",
    "    results = predict(\n",
    "        models_loader=models_loader,\n",
    "        model_id=model_id,\n",
    "        benchmark=benchmark,\n",
    "        inference_batch_size=1,\n",
    "        task=\"title\",\n",
    "        use_images=True,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=10\n",
    "    )\n",
    "\n",
    "calculate_blue_and_rouge_scores(benchmark=benchmark, task=\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335a7af",
   "metadata": {},
   "source": [
    "## `Qwen2.5-VL-3B-Instruct` is better in predicting title than `llava-1.5-7b-hf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcce52",
   "metadata": {},
   "source": [
    "# Try to remove image modality and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05452b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:Loaded 1085 cached results for Qwen2.5-VL-3B-Instruct\n",
      "INFO:utils:Predicting title for 0 samples\n",
      "INFO:utils:Saved 1085 predictions for Qwen2.5-VL-3B-Instruct to data/results/Qwen2.5-VL-3B-Instruct_title_no_images_predictions.json\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-VL-3B-Instruct</th>\n",
       "      <td>0.073</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              BLEU  ROUGE-L\n",
       "Model                                      \n",
       "Qwen/Qwen2.5-VL-3B-Instruct  0.073    0.355"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = QWEN_2_5_VL_MODEL_ID\n",
    "model_id_name = model_id.split(\"/\")[-1]\n",
    "results = predict(\n",
    "    models_loader=models_loader,\n",
    "    model_id=model_id,\n",
    "    benchmark=benchmark,\n",
    "    inference_batch_size=1,\n",
    "    task=\"title\",\n",
    "    use_images=False,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "\n",
    "calculate_blue_and_rouge_scores(benchmark=benchmark, task=\"title\", use_images=False, models_to_eval=[model_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abce6c",
   "metadata": {},
   "source": [
    "# Conclusion: BLUE and ROUGE decreased when I disabled image modality!\n",
    "\n",
    "## `-> image matters`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5448a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a12e8b",
   "metadata": {},
   "source": [
    "## Task 2.2\n",
    "- Input: title, main_category, n_words** and image\n",
    "- Output: description\n",
    "\n",
    "[!] **Since the ground truth description can be of any length, predicting a random description is not really fair (blue and rough will fail). To this end, I also specify the desired length of the description (in terms of the number of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a10149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils:Loaded 1085 cached results for llava-1.5-7b-hf\n",
      "INFO:utils:Predicting description for 0 samples\n",
      "INFO:utils:Saved 1085 predictions for llava-1.5-7b-hf to data/results/llava-1.5-7b-hf_description_predictions.json\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llava-hf/llava-1.5-7b-hf</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qwen/Qwen2.5-VL-3B-Instruct</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             BLEU  ROUGE-L\n",
       "Model                                     \n",
       "llava-hf/llava-1.5-7b-hf     0.03    0.222\n",
       "Qwen/Qwen2.5-VL-3B-Instruct  0.04    0.236"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_id in [LLAVA_MODEL_ID]: # QWEN_2_5_VL_MODEL_ID\n",
    "\n",
    "    model_id_name = model_id.split(\"/\")[-1]\n",
    "    results = predict(\n",
    "        models_loader=models_loader,\n",
    "        model_id=model_id,\n",
    "        benchmark=benchmark,\n",
    "        inference_batch_size=1,\n",
    "        task=\"description\",\n",
    "        use_images=True,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=200  # some descriptions can be quite long\n",
    "    )\n",
    "\n",
    "calculate_blue_and_rouge_scores(benchmark=benchmark, task=\"description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4496d1",
   "metadata": {},
   "source": [
    "## `Qwen2.5-VL-3B-Instruct` is slightly better in predicting description than `llava-1.5-7b-hf`\n",
    "*but these metrics really show nothing, since so many descriptions can be totally fine. Blue and rouge cannot evaluate it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be82b6",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "`Qwen/Qwen2.5-VL-3B-Instruct` outperformed `llava-hf/llava-1.5-7b-hf` again on both BLUE and ROUGE-L scores for both title prediction and description prediction tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b9694",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d1d8b",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge (via `Qwen/Qwen3-0.6B` -- poor choice, but fast and cheap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57d35018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from consts import LLM_AS_A_JUDGE_PROMPT_TEMPLATE, LLM_AS_A_JUDGE_MODEL_ID\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(LLM_AS_A_JUDGE_MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_AS_A_JUDGE_MODEL_ID)\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87fdbbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping evaluation of llava-1.5-7b-hf on title because it already exists\n",
      "Skipping evaluation of Qwen2.5-VL-3B-Instruct on title because it already exists\n",
      "Skipping evaluation of llava-1.5-7b-hf on description because it already exists\n",
      "Skipping evaluation of Qwen2.5-VL-3B-Instruct on description because it already exists\n"
     ]
    }
   ],
   "source": [
    "TASKS = [\"title\", \"description\"]\n",
    "MODEL_IDS = [LLAVA_MODEL_ID, QWEN_2_5_VL_MODEL_ID]\n",
    "\n",
    "\n",
    "def parse_llm_evaluation(evaluation_scores: list[str]) -> list[float]:\n",
    "    scores: list[float] = []\n",
    "    for score in evaluation_scores:\n",
    "        score_splitted = score.split()\n",
    "        value_parsed: bool = False\n",
    "        for s in score_splitted:\n",
    "            try:\n",
    "                score_value = float(s)\n",
    "                scores.append(score_value)\n",
    "                value_parsed = True\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if not value_parsed:\n",
    "            scores.append(0)\n",
    "            print(f\"WARNING: Failed to parse score: {score}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "for task in TASKS:\n",
    "    for model_id in MODEL_IDS:\n",
    "        model_id_name = model_id.split(\"/\")[-1]\n",
    "        llm_eval_scores_fp: str = f\"data/results/llm_eval_{model_id_name}_{task}.jsonl\"\n",
    "        if os.path.exists(llm_eval_scores_fp):\n",
    "            print(f\"Skipping evaluation of {model_id_name} on {task} because it already exists\")\n",
    "            continue\n",
    "\n",
    "        llm_evaluation_scores: list[str] = []\n",
    "        results_fp: str = f\"data/results/{model_id_name}_{task}_predictions.json\"\n",
    "        results: dict[str, str] = load_results(results_fp) # results = dict[parent_asin] = predicted_<task>\n",
    "        results_list: list[str] = list(results.values())\n",
    "        results_parent_asins: list[str] = list(results.keys())\n",
    "\n",
    "        inference_batch_size: int = 64\n",
    "        for i in tqdm(range(0, len(results), inference_batch_size), desc=f\"Evaluating {model_id_name} on {task}\"):\n",
    "            batch = results_list[i:i+inference_batch_size]\n",
    "            parent_asins = results_parent_asins[i:i+inference_batch_size]\n",
    "            chats = [\n",
    "                [ \n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": LLM_AS_A_JUDGE_PROMPT_TEMPLATE.format(\n",
    "                            task=task,\n",
    "                            original=benchmark.loc[benchmark[\"parent_asin\"] == parent_asin, task].values[0],\n",
    "                            predicted=results[parent_asin]\n",
    "                        )\n",
    "                    } \n",
    "                ] for parent_asin in parent_asins\n",
    "            ]\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                chats,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                enable_thinking=False\n",
    "            ).to(model.device)\n",
    "\n",
    "            outputs = model.generate(**inputs, max_new_tokens=5, do_sample=False)\n",
    "            decoded_results = tokenizer.batch_decode([\n",
    "                output[inputs[\"input_ids\"].shape[-1]:]\n",
    "                for output in outputs\n",
    "            ], skip_special_tokens=True)\n",
    "            llm_evaluation_scores.extend([\n",
    "                {\"parent_asin\": parent_asin, \"score\": score}\n",
    "                for parent_asin, score in zip(parent_asins, parse_llm_evaluation(decoded_results))\n",
    "            ])\n",
    "\n",
    "        # save llm score to /data/results/llm_eval_{model_id_name}_{task}.jsonl (one line = JSON per parent_asin and score)\n",
    "        with open(llm_eval_scores_fp, \"w\") as f:\n",
    "            for entry in llm_evaluation_scores:\n",
    "                f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718b4dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                  |   Average LLM Score (title) |\n",
      "|------------------------|-----------------------------|\n",
      "| llava-1.5-7b-hf        |                       7.597 |\n",
      "| Qwen2.5-VL-3B-Instruct |                       8.106 |\n",
      "\n",
      "| Model                  |   Average LLM Score (description) |\n",
      "|------------------------|-----------------------------------|\n",
      "| llava-1.5-7b-hf        |                             7.021 |\n",
      "| Qwen2.5-VL-3B-Instruct |                             7.576 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in TASKS:\n",
    "    table = []\n",
    "    headers = [\"Model\", f\"Average LLM Score ({task})\"]\n",
    "    for model_id in MODEL_IDS:\n",
    "        model_id_name = model_id.split(\"/\")[-1]\n",
    "        llm_eval_scores_fp: str = f\"data/results/llm_eval_{model_id_name}_{task}.jsonl\"\n",
    "        with open(llm_eval_scores_fp, \"r\") as f:\n",
    "            llm_evaluation_scores = [json.loads(line) for line in f]\n",
    "            average_score = np.mean([e[\"score\"] for e in llm_evaluation_scores])\n",
    "            table.append([model_id_name, average_score])\n",
    "\n",
    "    print(tabulate(table, headers=headers, floatfmt=\".3f\", tablefmt=\"github\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d52aee",
   "metadata": {},
   "source": [
    "### Result: `Qwen2.5-VL-3B-Instruct` wins again (according to LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f208ecd",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
