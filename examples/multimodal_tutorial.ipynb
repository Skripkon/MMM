{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Models Tutorial\n",
    "## HSE 2025 - Introduction to Multi-Modal Learning\n",
    "\n",
    "This notebook introduces the concepts and practical implementation of multi-modal machine learning using the MMM framework.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand multi-modal learning principles\n",
    "- Learn to work with different data modalities\n",
    "- Implement and train multi-modal classifiers\n",
    "- Explore different fusion strategies\n",
    "- Evaluate multi-modal models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision transformers matplotlib seaborn\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# MMM imports\n",
    "from models.multimodal_classifier import MultiModalClassifier\n",
    "from data.multimodal_dataset import MultiModalDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Multi-Modal Data\n",
    "\n",
    "Multi-modal learning involves processing and learning from multiple types of data simultaneously:\n",
    "\n",
    "- **Text**: Natural language (reviews, descriptions, captions)\n",
    "- **Images**: Visual content (photos, diagrams, charts)\n",
    "- **Audio**: Sound data (speech, music, environmental sounds)\n",
    "- **Video**: Temporal visual sequences\n",
    "- **Tabular**: Structured numerical data\n",
    "\n",
    "The key challenge is learning meaningful representations that capture relationships **across** modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create synthetic multi-modal data to understand the concepts\n",
    "\n",
    "def create_synthetic_dataset(n_samples=1000, n_classes=5):\n",
    "    \"\"\"\n",
    "    Create synthetic multi-modal dataset for demonstration.\n",
    "    \n",
    "    In real scenarios, you would:\n",
    "    - Extract text features using BERT/RoBERTa\n",
    "    - Extract image features using ResNet/ViT\n",
    "    - Extract audio features using spectrograms/MFCC\n",
    "    \"\"\"\n",
    "    \n",
    "    # Text features (simulating BERT embeddings)\n",
    "    text_features = torch.randn(n_samples, 768)\n",
    "    \n",
    "    # Image features (simulating ResNet features)\n",
    "    image_features = torch.randn(n_samples, 2048)\n",
    "    \n",
    "    # Audio features (simulating mel-spectrogram features)\n",
    "    audio_features = torch.randn(n_samples, 512)\n",
    "    \n",
    "    # Create correlated labels (some modalities are more informative for certain classes)\n",
    "    labels = torch.randint(0, n_classes, (n_samples,))\n",
    "    \n",
    "    # Add some correlation between modalities and labels\n",
    "    for i in range(n_classes):\n",
    "        mask = labels == i\n",
    "        # Make class i more distinctive in text features\n",
    "        text_features[mask] += torch.randn(mask.sum(), 768) * 0.5 + i\n",
    "        # Make class i more distinctive in image features  \n",
    "        image_features[mask] += torch.randn(mask.sum(), 2048) * 0.3 + i * 0.5\n",
    "        # Make class i more distinctive in audio features\n",
    "        audio_features[mask] += torch.randn(mask.sum(), 512) * 0.4 + i * 0.3\n",
    "    \n",
    "    return {\n",
    "        'text': text_features,\n",
    "        'image': image_features, \n",
    "        'audio': audio_features,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Create datasets\n",
    "train_data = create_synthetic_dataset(n_samples=800, n_classes=5)\n",
    "val_data = create_synthetic_dataset(n_samples=200, n_classes=5)\n",
    "\n",
    "print(\"Dataset created:\")\n",
    "print(f\"Training samples: {len(train_data['labels'])}\")\n",
    "print(f\"Validation samples: {len(val_data['labels'])}\")\n",
    "print(f\"Number of classes: 5\")\n",
    "print(f\"Text feature dim: {train_data['text'].shape[1]}\")\n",
    "print(f\"Image feature dim: {train_data['image'].shape[1]}\")\n",
    "print(f\"Audio feature dim: {train_data['audio'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Modal Fusion Strategies\n",
    "\n",
    "There are several ways to combine information from different modalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore different fusion strategies\n",
    "\n",
    "fusion_methods = ['concat', 'attention', 'sum']\n",
    "models = {}\n",
    "\n",
    "for fusion_method in fusion_methods:\n",
    "    model = MultiModalClassifier(\n",
    "        num_classes=5,\n",
    "        text_dim=768,\n",
    "        image_dim=2048, \n",
    "        audio_dim=512,\n",
    "        hidden_dim=256,\n",
    "        fusion_method=fusion_method,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    models[fusion_method] = model\n",
    "    \n",
    "    # Count parameters\n",
    "    param_count = model.get_parameter_count()\n",
    "    print(f\"{fusion_method.capitalize()} Fusion:\")\n",
    "    print(f\"  Parameters: {param_count['total']:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Concatenation Fusion\n",
    "Simply concatenates all modality representations and passes through a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concatenation fusion\n",
    "concat_model = models['concat']\n",
    "concat_model.eval()\n",
    "\n",
    "# Sample batch\n",
    "batch_size = 32\n",
    "sample_inputs = {\n",
    "    'text': train_data['text'][:batch_size],\n",
    "    'image': train_data['image'][:batch_size],\n",
    "    'audio': train_data['audio'][:batch_size]\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = concat_model(sample_inputs)\n",
    "    \n",
    "print(\"Concatenation Fusion Results:\")\n",
    "print(f\"Input shapes:\")\n",
    "for modality, data in sample_inputs.items():\n",
    "    print(f\"  {modality}: {data.shape}\")\n",
    "    \n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  Fused features: {outputs['fused_features'].shape}\")\n",
    "print(f\"  Logits: {outputs['logits'].shape}\")\n",
    "print(f\"  Predictions: {outputs['predictions'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Attention Fusion\n",
    "Uses attention mechanism to learn which modalities are most important for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention fusion\n",
    "attention_model = models['attention']\n",
    "attention_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = attention_model(sample_inputs)\n",
    "    \n",
    "print(\"Attention Fusion Results:\")\n",
    "print(f\"Fused features shape: {outputs['fused_features'].shape}\")\n",
    "print(f\"Predictions shape: {outputs['predictions'].shape}\")\n",
    "\n",
    "# The attention model learns to weight different modalities\n",
    "print(\"\\nAttention mechanism learns optimal modality weights for each sample!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Multi-Modal Models\n",
    "\n",
    "Let's train our models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs=20, lr=0.001):\n",
    "    \"\"\"\n",
    "    Simple training loop for demonstration.\n",
    "    In practice, you'd use more sophisticated training utilities.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    batch_size = 64\n",
    "    n_train = len(train_data['labels'])\n",
    "    n_val = len(val_data['labels'])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(0, n_train, batch_size):\n",
    "            end_idx = min(i + batch_size, n_train)\n",
    "            \n",
    "            inputs = {\n",
    "                'text': train_data['text'][i:end_idx],\n",
    "                'image': train_data['image'][i:end_idx],\n",
    "                'audio': train_data['audio'][i:end_idx]\n",
    "            }\n",
    "            labels = train_data['labels'][i:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs['logits'], labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_val, batch_size):\n",
    "                end_idx = min(i + batch_size, n_val)\n",
    "                \n",
    "                inputs = {\n",
    "                    'text': val_data['text'][i:end_idx],\n",
    "                    'image': val_data['image'][i:end_idx], \n",
    "                    'audio': val_data['audio'][i:end_idx]\n",
    "                }\n",
    "                labels = val_data['labels'][i:end_idx]\n",
    "                \n",
    "                predictions = model.predict(inputs)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        avg_loss = epoch_loss / (n_train // batch_size)\n",
    "        accuracy = correct / n_val\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:2d}: Loss = {avg_loss:.4f}, Val Acc = {accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "print(\"Training models with different fusion strategies...\")\n",
    "print(\"This may take a few minutes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "results = {}\n",
    "\n",
    "for fusion_method, model in models.items():\n",
    "    print(f\"Training {fusion_method} fusion model...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_data, val_data, epochs=15)\n",
    "    results[fusion_method] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'final_accuracy': val_accuracies[-1]\n",
    "    }\n",
    "    print(f\"Final validation accuracy: {val_accuracies[-1]:.4f}\\n\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training losses\n",
    "for fusion_method, result in results.items():\n",
    "    ax1.plot(result['train_losses'], label=f'{fusion_method.capitalize()} Fusion')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Validation accuracies\n",
    "for fusion_method, result in results.items():\n",
    "    ax2.plot(result['val_accuracies'], label=f'{fusion_method.capitalize()} Fusion')\n",
    "ax2.set_title('Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final accuracy comparison\n",
    "print(\"Final Validation Accuracies:\")\n",
    "for fusion_method, result in results.items():\n",
    "    print(f\"{fusion_method.capitalize():>10}: {result['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handling Missing Modalities\n",
    "\n",
    "One of the key advantages of multi-modal models is their ability to handle missing modalities gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different modality combinations\n",
    "best_model = models['attention']  # Use the attention model\n",
    "best_model.eval()\n",
    "\n",
    "test_batch = {\n",
    "    'text': val_data['text'][:10],\n",
    "    'image': val_data['image'][:10],\n",
    "    'audio': val_data['audio'][:10]\n",
    "}\n",
    "test_labels = val_data['labels'][:10]\n",
    "\n",
    "modality_combinations = [\n",
    "    (['text', 'image', 'audio'], \"All Modalities\"),\n",
    "    (['text', 'image'], \"Text + Image\"),\n",
    "    (['text', 'audio'], \"Text + Audio\"),\n",
    "    (['image', 'audio'], \"Image + Audio\"),\n",
    "    (['text'], \"Text Only\"),\n",
    "    (['image'], \"Image Only\"),\n",
    "    (['audio'], \"Audio Only\")\n",
    "]\n",
    "\n",
    "print(\"Performance with different modality combinations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for modalities, description in modality_combinations:\n",
    "        # Create input with only specified modalities\n",
    "        partial_input = {mod: test_batch[mod] for mod in modalities}\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = best_model.predict(partial_input)\n",
    "        accuracy = (predictions == test_labels).float().mean().item()\n",
    "        \n",
    "        print(f\"{description:>20}: {accuracy:.3f} accuracy\")\n",
    "\n",
    "print(\"\\nüí° Notice how the model maintains reasonable performance even with missing modalities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the learned representations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_features(model, data, labels, title=\"Feature Visualization\"):\n",
    "    \"\"\"Visualize learned features using t-SNE.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = {\n",
    "            'text': data['text'][:300],  # Use subset for visualization\n",
    "            'image': data['image'][:300],\n",
    "            'audio': data['audio'][:300]\n",
    "        }\n",
    "        outputs = model(inputs)\n",
    "        features = outputs['fused_features'].numpy()\n",
    "        labels_subset = labels[:300].numpy()\n",
    "    \n",
    "    # Apply t-SNE for 2D visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                         c=labels_subset, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f'{title} - Learned Feature Space (t-SNE)')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize features from the best model\n",
    "visualize_features(best_model, val_data, val_data['labels'], \n",
    "                  \"Multi-Modal Attention Fusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Applications\n",
    "\n",
    "Multi-modal learning has many practical applications:\n",
    "\n",
    "### 8.1 Sentiment Analysis\n",
    "- **Text**: Review text\n",
    "- **Image**: Product images\n",
    "- **Audio**: Voice tone in video reviews\n",
    "\n",
    "### 8.2 Medical Diagnosis\n",
    "- **Text**: Patient symptoms and history\n",
    "- **Image**: X-rays, MRI scans\n",
    "- **Audio**: Heart sounds, breathing patterns\n",
    "\n",
    "### 8.3 Content Moderation\n",
    "- **Text**: Captions and comments\n",
    "- **Image**: Visual content\n",
    "- **Audio**: Speech in videos\n",
    "\n",
    "### 8.4 Autonomous Driving\n",
    "- **Image**: Camera feeds\n",
    "- **Audio**: Engine sounds, sirens\n",
    "- **Sensor**: LiDAR, GPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulate a content moderation scenario\n",
    "print(\"üîç Content Moderation Example\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate different types of content\n",
    "content_types = {\n",
    "    0: \"Safe Content\",\n",
    "    1: \"Questionable Content\", \n",
    "    2: \"Inappropriate Content\",\n",
    "    3: \"Spam Content\",\n",
    "    4: \"Harmful Content\"\n",
    "}\n",
    "\n",
    "# Create a specialized model for content moderation\n",
    "moderation_model = MultiModalClassifier(\n",
    "    num_classes=5,\n",
    "    text_dim=768,   # Text from captions/comments\n",
    "    image_dim=2048, # Visual content features\n",
    "    audio_dim=512,  # Audio from videos\n",
    "    fusion_method=\"attention\",\n",
    "    hidden_dim=512\n",
    ")\n",
    "\n",
    "print(f\"Content Moderation Model:\")\n",
    "print(f\"- Classes: {list(content_types.values())}\")\n",
    "print(f\"- Parameters: {moderation_model.get_parameter_count()['total']:,}\")\n",
    "print(f\"- Fusion Method: Attention (learns which modality is most indicative)\")\n",
    "\n",
    "# In practice, you would:\n",
    "# 1. Extract text features using BERT from captions/comments\n",
    "# 2. Extract image features using Vision Transformers\n",
    "# 3. Extract audio features from video soundtracks\n",
    "# 4. Train on labeled content moderation datasets\n",
    "# 5. Deploy for real-time content screening\n",
    "\n",
    "print(\"\\n‚ú® This demonstrates the power of multi-modal learning!\")\n",
    "print(\"   Each modality provides complementary information for better decisions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways and Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "1. **Multi-modal Fusion**: Different strategies (concat, attention, sum) for combining modalities\n",
    "2. **Robustness**: Models can handle missing modalities gracefully\n",
    "3. **Flexibility**: Framework supports various architectures and use cases\n",
    "4. **Performance**: Attention fusion often performs best by learning optimal weights\n",
    "\n",
    "### Best Practices:\n",
    "- **Data Quality**: Ensure all modalities are properly preprocessed\n",
    "- **Feature Engineering**: Use domain-appropriate feature extractors\n",
    "- **Fusion Strategy**: Choose based on your specific task and data\n",
    "- **Evaluation**: Test with different modality combinations\n",
    "- **Interpretability**: Use attention weights to understand model decisions\n",
    "\n",
    "### Next Steps for HSE Students:\n",
    "1. **Practice**: Try the framework with real datasets\n",
    "2. **Experiment**: Create custom fusion mechanisms\n",
    "3. **Apply**: Use for your course projects\n",
    "4. **Extend**: Add new modalities (video, sensor data)\n",
    "5. **Research**: Explore latest multi-modal architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéì Congratulations! You've completed the Multi-Modal Learning Tutorial\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüìö What you've accomplished:\")\n",
    "print(\"  ‚úÖ Understood multi-modal learning concepts\")\n",
    "print(\"  ‚úÖ Implemented different fusion strategies\")\n",
    "print(\"  ‚úÖ Trained and evaluated multi-modal models\")\n",
    "print(\"  ‚úÖ Explored handling missing modalities\")\n",
    "print(\"  ‚úÖ Visualized learned representations\")\n",
    "print(\"  ‚úÖ Considered real-world applications\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for advanced multi-modal AI research!\")\n",
    "print(\"\\nüìñ Additional Resources:\")\n",
    "print(\"  - MMM Documentation: docs/\")\n",
    "print(\"  - More Examples: examples/\")\n",
    "print(\"  - Configuration: configs/\")\n",
    "print(\"  - HSE AI Lab: ai-lab@hse.ru\")\n",
    "\n",
    "print(\"\\nüéØ HSE 2025 - Multi-Modal Models Framework\")\n",
    "print(\"   Happy Learning! üß†‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}